# examples/benchmark_pipeline.py
"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫.

–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π:
1. –ù–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–±–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
2. –ù–∞ –¥–∞–Ω–Ω—ã—Ö, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö AutoML Data Forge

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
- –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Ä–µ–≥—Ä–µ—Å—Å–∏—è)
- –¢–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent))

import warnings
warnings.filterwarnings('ignore')

import time
from dataclasses import dataclass, field
from typing import Callable
from abc import ABC, abstractmethod

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, mean_squared_error, mean_absolute_error, r2_score
)
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline as SklearnPipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

# –ú–æ–¥–µ–ª–∏
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.svm import SVC

# –ù–∞—à–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞
from automl_data import AutoForge, ForgeResult


# ==================== –î–∞—Ç–∞–∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ====================

@dataclass
class MetricsResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ–¥–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
    accuracy: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    f1: float = 0.0
    roc_auc: float = 0.0
    # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    mse: float = 0.0
    mae: float = 0.0
    r2: float = 0.0
    # –ú–µ—Ç–∞
    train_time: float = 0.0
    preprocessing_time: float = 0.0
    
    def to_dict(self) -> dict:
        return {k: round(v, 4) for k, v in self.__dict__.items()}


@dataclass
class ExperimentResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–¥–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
    dataset_name: str
    task_type: str
    baseline_metrics: MetricsResult
    forge_metrics: MetricsResult
    improvement: dict = field(default_factory=dict)
    
    def __post_init__(self):
        """–í—ã—á–∏—Å–ª—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ"""
        if self.task_type == "classification":
            for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:
                baseline = getattr(self.baseline_metrics, metric)
                forge = getattr(self.forge_metrics, metric)
                if baseline > 0:
                    self.improvement[metric] = ((forge - baseline) / baseline) * 100
                else:
                    self.improvement[metric] = 0.0
        else:  # regression
            # –î–ª—è MSE/MAE –º–µ–Ω—å—à–µ = –ª—É—á—à–µ
            for metric in ['mse', 'mae']:
                baseline = getattr(self.baseline_metrics, metric)
                forge = getattr(self.forge_metrics, metric)
                if baseline > 0:
                    self.improvement[metric] = ((baseline - forge) / baseline) * 100
            # –î–ª—è R2 –±–æ–ª—å—à–µ = –ª—É—á—à–µ
            baseline_r2 = self.baseline_metrics.r2
            forge_r2 = self.forge_metrics.r2
            if baseline_r2 > 0:
                self.improvement['r2'] = ((forge_r2 - baseline_r2) / abs(baseline_r2)) * 100


# ==================== –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ ====================

class DatasetGenerator(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    
    @abstractmethod
    def generate(self) -> tuple[pd.DataFrame, str, str]:
        """
        Returns:
            (DataFrame, target_column, task_type)
        """
        pass
    
    @property
    @abstractmethod
    def name(self) -> str:
        pass


class TabularClassificationDataset(DatasetGenerator):
    """–¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(
        self,
        n_samples: int = 1000,
        n_features: int = 10,
        n_categories: int = 3,
        missing_rate: float = 0.1,
        imbalance_ratio: float = 0.3,
        noise_level: float = 0.1
    ):
        self.n_samples = n_samples
        self.n_features = n_features
        self.n_categories = n_categories
        self.missing_rate = missing_rate
        self.imbalance_ratio = imbalance_ratio
        self.noise_level = noise_level
    
    @property
    def name(self) -> str:
        return f"Tabular_Classification_{self.n_samples}x{self.n_features}"
    
    def generate(self) -> tuple[pd.DataFrame, str, str]:
        np.random.seed(42)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        data = {}
        
        # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        n_numeric = self.n_features // 2
        for i in range(n_numeric):
            data[f'numeric_{i}'] = np.random.randn(self.n_samples)
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã
            outlier_idx = np.random.choice(self.n_samples, size=int(self.n_samples * 0.02), replace=False)
            data[f'numeric_{i}'][outlier_idx] *= 10
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        n_categorical = self.n_features - n_numeric
        categories = ['cat_A', 'cat_B', 'cat_C', 'cat_D', 'cat_E']
        for i in range(n_categorical):
            data[f'category_{i}'] = np.random.choice(
                categories[:self.n_categories], 
                self.n_samples
            )
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (–Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
        n_positive = int(self.n_samples * self.imbalance_ratio)
        n_negative = self.n_samples - n_positive
        target = np.array([0] * n_negative + [1] * n_positive)
        np.random.shuffle(target)
        data['target'] = target
        
        df = pd.DataFrame(data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        for col in df.columns:
            if col != 'target':
                missing_idx = np.random.choice(
                    self.n_samples, 
                    size=int(self.n_samples * self.missing_rate), 
                    replace=False
                )
                df.loc[missing_idx, col] = np.nan
        
        return df, 'target', 'classification'


class TabularRegressionDataset(DatasetGenerator):
    """–¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"""
    
    def __init__(
        self,
        n_samples: int = 1000,
        n_features: int = 8,
        missing_rate: float = 0.1,
        noise_level: float = 0.2
    ):
        self.n_samples = n_samples
        self.n_features = n_features
        self.missing_rate = missing_rate
        self.noise_level = noise_level
    
    @property
    def name(self) -> str:
        return f"Tabular_Regression_{self.n_samples}x{self.n_features}"
    
    def generate(self) -> tuple[pd.DataFrame, str, str]:
        np.random.seed(42)
        
        data = {}
        
        # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for i in range(self.n_features - 2):
            data[f'feature_{i}'] = np.random.randn(self.n_samples)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
        data['category_1'] = np.random.choice(['low', 'medium', 'high'], self.n_samples)
        data['category_2'] = np.random.choice(['type_A', 'type_B'], self.n_samples)
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ + —à—É–º)
        target = (
            2 * data['feature_0'] + 
            0.5 * data['feature_1'] - 
            1.5 * data['feature_2'] +
            np.random.randn(self.n_samples) * self.noise_level * 5
        )
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–ª–∏—è–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        target += np.where(np.array(data['category_1']) == 'high', 3, 0)
        data['target'] = target
        
        df = pd.DataFrame(data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        for col in df.columns:
            if col != 'target':
                missing_idx = np.random.choice(
                    self.n_samples, 
                    size=int(self.n_samples * self.missing_rate), 
                    replace=False
                )
                df.loc[missing_idx, col] = np.nan
        
        return df, 'target', 'regression'


class TextClassificationDataset(DatasetGenerator):
    """–¢–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (sentiment analysis)"""
    
    def __init__(self, n_samples: int = 500, imbalance_ratio: float = 0.3):
        self.n_samples = n_samples
        self.imbalance_ratio = imbalance_ratio
    
    @property
    def name(self) -> str:
        return f"Text_Sentiment_{self.n_samples}"
    
    def generate(self) -> tuple[pd.DataFrame, str, str]:
        np.random.seed(42)
        
        positive_templates = [
            "This product is absolutely amazing! I love it!",
            "Great quality and fast shipping. Highly recommend!",
            "Best purchase I've ever made. Five stars!",
            "Exceeded my expectations. Will buy again!",
            "Perfect! Exactly what I was looking for.",
            "Wonderful experience, very satisfied customer.",
            "Outstanding product, works like a charm!",
            "Love love love this! So happy with my purchase.",
            "Fantastic quality for the price. Very impressed!",
            "Couldn't be happier! This is exactly what I needed.",
        ]
        
        negative_templates = [
            "Terrible product. Complete waste of money.",
            "Very disappointed. Does not work as advertised.",
            "Poor quality, broke after one day.",
            "Worst purchase ever. Do not buy this!",
            "Horrible experience. Returning immediately.",
            "Not worth the money. Very cheaply made.",
            "Disappointed with this product. Expected better.",
            "Save your money. This product is garbage.",
            "Awful quality. Falls apart easily.",
            "Regret buying this. Total disappointment.",
        ]
        
        n_positive = int(self.n_samples * (1 - self.imbalance_ratio))
        n_negative = self.n_samples - n_positive
        
        texts = (
            [np.random.choice(positive_templates) for _ in range(n_positive)] +
            [np.random.choice(negative_templates) for _ in range(n_negative)]
        )
        labels = [1] * n_positive + [0] * n_negative
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        def add_noise(text):
            modifications = [
                lambda t: t.lower(),
                lambda t: t.upper(),
                lambda t: "  " + t + "  ",  # –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
                lambda t: t.replace("!", "!!!"),
                lambda t: t,  # –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            ]
            return np.random.choice(modifications)(text)
        
        texts = [add_noise(t) for t in texts]
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
        indices = np.random.permutation(len(texts))
        texts = [texts[i] for i in indices]
        labels = [labels[i] for i in indices]
        
        return pd.DataFrame({
            'text': texts,
            'sentiment': labels
        }), 'sentiment', 'text_classification'


class ImbalancedDataset(DatasetGenerator):
    """–°–∏–ª—å–Ω–æ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (95/5)"""
    
    def __init__(self, n_samples: int = 2000):
        self.n_samples = n_samples
    
    @property
    def name(self) -> str:
        return f"Highly_Imbalanced_{self.n_samples}"
    
    def generate(self) -> tuple[pd.DataFrame, str, str]:
        np.random.seed(42)
        
        n_minority = int(self.n_samples * 0.05)
        n_majority = self.n_samples - n_minority
        
        # –ú–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å
        X_majority = np.random.randn(n_majority, 5)
        
        # –ú–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å (—Å–º–µ—â—ë–Ω–Ω—ã–π)
        X_minority = np.random.randn(n_minority, 5) + 2
        
        X = np.vstack([X_majority, X_minority])
        y = np.array([0] * n_majority + [1] * n_minority)
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
        indices = np.random.permutation(len(y))
        X = X[indices]
        y = y[indices]
        
        df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(5)])
        df['target'] = y
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        for col in df.columns[:-1]:
            missing_idx = np.random.choice(len(df), size=int(len(df) * 0.08), replace=False)
            df.loc[missing_idx, col] = np.nan
        
        return df, 'target', 'classification'


# ==================== –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (baseline) ====================

def baseline_preprocess(
    df: pd.DataFrame, 
    target: str, 
    task_type: str
) -> tuple[pd.DataFrame, pd.Series]:
    """
    –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (baseline).
    –¢–æ, —á—Ç–æ –æ–±—ã—á–Ω–æ –¥–µ–ª–∞—é—Ç –≤—Ä—É—á–Ω—É—é.
    """
    df = df.copy()
    y = df.pop(target)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # –ò–º–ø—å—é—Ç–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö
    if numeric_cols:
        imputer = SimpleImputer(strategy='median')
        df[numeric_cols] = imputer.fit_transform(df[numeric_cols])
    
    # –ò–º–ø—å—é—Ç–∞—Ü–∏—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
    if categorical_cols:
        for col in categorical_cols:
            df[col] = df[col].fillna('missing')
        
        # One-hot encoding
        df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    if numeric_cols:
        scaler = StandardScaler()
        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
    
    return df, y


def baseline_text_preprocess(
    df: pd.DataFrame,
    text_col: str,
    target: str
) -> tuple[pd.DataFrame, pd.Series]:
    """–ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (TF-IDF)"""
    from sklearn.feature_extraction.text import TfidfVectorizer
    
    df = df.copy()
    y = df[target]
    texts = df[text_col].fillna('')
    
    vectorizer = TfidfVectorizer(max_features=500, stop_words='english')
    X = vectorizer.fit_transform(texts)
    
    return pd.DataFrame(X.toarray()), y


# ==================== –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ ====================

def train_and_evaluate(
    X_train: pd.DataFrame,
    X_test: pd.DataFrame,
    y_train: pd.Series,
    y_test: pd.Series,
    task_type: str,
    model=None
) -> MetricsResult:
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏"""
    
    result = MetricsResult()
    
    # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if model is None:
        if task_type in ['classification', 'text_classification']:
            model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        else:
            model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    
    # –û–±—É—á–∞–µ–º
    start_time = time.time()
    model.fit(X_train, y_train)
    result.train_time = time.time() - start_time
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
    y_pred = model.predict(X_test)
    
    if task_type in ['classification', 'text_classification']:
        result.accuracy = accuracy_score(y_test, y_pred)
        result.precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        result.recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        result.f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # ROC-AUC (—Ç–æ–ª—å–∫–æ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
        if len(np.unique(y_test)) == 2 and hasattr(model, 'predict_proba'):
            try:
                y_proba = model.predict_proba(X_test)[:, 1]
                result.roc_auc = roc_auc_score(y_test, y_proba)
            except Exception:
                result.roc_auc = 0.0
    else:
        result.mse = mean_squared_error(y_test, y_pred)
        result.mae = mean_absolute_error(y_test, y_pred)
        result.r2 = r2_score(y_test, y_pred)
    
    return result


# ==================== –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –±–µ–Ω—á–º–∞—Ä–∫–∞ ====================

class BenchmarkPipeline:
    """
    –ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è baseline vs AutoForge.
    """
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.results: list[ExperimentResult] = []
    
    def log(self, message: str):
        if self.verbose:
            print(message)
    
    def run_experiment(
        self,
        dataset_generator: DatasetGenerator,
        forge_config: dict = None
    ) -> ExperimentResult:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç"""
        
        self.log(f"\n{'='*60}")
        self.log(f"üìä Dataset: {dataset_generator.name}")
        self.log('='*60)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        df, target, task_type = dataset_generator.generate()
        self.log(f"   Shape: {df.shape}, Task: {task_type}")
        self.log(f"   Missing values: {df.isnull().sum().sum()}")
        
        if task_type in ['classification', 'text_classification']:
            class_dist = df[target].value_counts()
            self.log(f"   Class distribution: {dict(class_dist)}")
        
        # ============ BASELINE ============
        self.log(f"\nüîπ Running BASELINE preprocessing...")
        
        start_time = time.time()
        
        if task_type == 'text_classification':
            # –ù–∞—Ö–æ–¥–∏–º —Ç–µ–∫—Å—Ç–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É
            text_col = [c for c in df.columns if c != target][0]
            X_baseline, y_baseline = baseline_text_preprocess(df, text_col, target)
        else:
            X_baseline, y_baseline = baseline_preprocess(df, target, task_type)
        
        baseline_preprocess_time = time.time() - start_time
        
        # Train/test split
        X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(
            X_baseline, y_baseline, test_size=0.2, random_state=42, stratify=y_baseline if task_type != 'regression' else None
        )
        
        baseline_metrics = train_and_evaluate(
            X_train_b, X_test_b, y_train_b, y_test_b, task_type
        )
        baseline_metrics.preprocessing_time = baseline_preprocess_time
        
        self.log(f"   Preprocessing time: {baseline_preprocess_time:.2f}s")
        self.log(f"   Training time: {baseline_metrics.train_time:.2f}s")
        
        # ============ AUTOFORGE ============
        self.log(f"\nüî∏ Running AUTOFORGE preprocessing...")
        
        start_time = time.time()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è AutoForge
        config = {
            'target': target,
            'verbose': False,
        }
        
        if task_type == 'text_classification':
            text_col = [c for c in df.columns if c != target][0]
            config['text_column'] = text_col
        
        if forge_config:
            config.update(forge_config)
        
        try:
            forge = AutoForge(**config)
            result = forge.fit_transform(df)
            
            forge_preprocess_time = time.time() - start_time
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–ª–∏—Ç—ã
            X_train_f, X_test_f, y_train_f, y_test_f = result.get_splits(test_size=0.2)
            
            forge_metrics = train_and_evaluate(
                X_train_f, X_test_f, y_train_f, y_test_f, task_type
            )
            forge_metrics.preprocessing_time = forge_preprocess_time
            
            self.log(f"   Preprocessing time: {forge_preprocess_time:.2f}s")
            self.log(f"   Training time: {forge_metrics.train_time:.2f}s")
            
        except Exception as e:
            self.log(f"   ‚ö†Ô∏è AutoForge failed: {e}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º baseline –∫–∞–∫ fallback
            forge_metrics = baseline_metrics
        
        # ============ –°–†–ê–í–ù–ï–ù–ò–ï ============
        experiment = ExperimentResult(
            dataset_name=dataset_generator.name,
            task_type=task_type,
            baseline_metrics=baseline_metrics,
            forge_metrics=forge_metrics
        )
        
        self._print_comparison(experiment)
        
        self.results.append(experiment)
        return experiment
    
    def _print_comparison(self, exp: ExperimentResult):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫"""
        self.log(f"\nüìà RESULTS COMPARISON:")
        self.log("-" * 50)
        
        if exp.task_type in ['classification', 'text_classification']:
            metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
        else:
            metrics = ['mse', 'mae', 'r2']
        
        self.log(f"{'Metric':<15} {'Baseline':>12} {'AutoForge':>12} {'Change':>12}")
        self.log("-" * 50)
        
        for metric in metrics:
            baseline_val = getattr(exp.baseline_metrics, metric)
            forge_val = getattr(exp.forge_metrics, metric)
            change = exp.improvement.get(metric, 0)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            if metric in ['mse', 'mae']:
                b_str = f"{baseline_val:.4f}"
                f_str = f"{forge_val:.4f}"
            else:
                b_str = f"{baseline_val:.4f}"
                f_str = f"{forge_val:.4f}"
            
            # –¶–≤–µ—Ç–æ–≤–æ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
            if change > 0:
                change_str = f"+{change:.1f}% ‚úÖ"
            elif change < 0:
                change_str = f"{change:.1f}% ‚ùå"
            else:
                change_str = f"{change:.1f}%"
            
            self.log(f"{metric:<15} {b_str:>12} {f_str:>12} {change_str:>12}")
    
    def run_all(self, datasets: list[DatasetGenerator] = None):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã"""
        
        if datasets is None:
            datasets = [
                TabularClassificationDataset(n_samples=1000, missing_rate=0.1),
                TabularClassificationDataset(n_samples=500, missing_rate=0.2),
                TabularRegressionDataset(n_samples=1000),
                ImbalancedDataset(n_samples=2000),
                TextClassificationDataset(n_samples=500),
            ]
        
        self.log("\n" + "="*60)
        self.log("üöÄ STARTING BENCHMARK PIPELINE")
        self.log("="*60)
        
        for dataset in datasets:
            try:
                self.run_experiment(dataset)
            except Exception as e:
                self.log(f"‚ùå Error in {dataset.name}: {e}")
        
        self._print_summary()
    
    def _print_summary(self):
        """–ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞"""
        self.log("\n" + "="*60)
        self.log("üìä FINAL SUMMARY")
        self.log("="*60)
        
        if not self.results:
            self.log("No results to summarize")
            return
        
        # –°—Ä–µ–¥–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è
        improvements = {
            'accuracy': [], 'precision': [], 'recall': [], 
            'f1': [], 'roc_auc': [], 'r2': []
        }
        
        for exp in self.results:
            for metric, value in exp.improvement.items():
                if metric in improvements:
                    improvements[metric].append(value)
        
        self.log("\nüìà Average improvements across all experiments:")
        for metric, values in improvements.items():
            if values:
                avg = np.mean(values)
                if avg > 0:
                    self.log(f"   {metric}: +{avg:.1f}% ‚úÖ")
                else:
                    self.log(f"   {metric}: {avg:.1f}%")
        
        # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.log("\nüìã Detailed results table:")
        self.log("-" * 80)
        self.log(f"{'Dataset':<35} {'Task':<15} {'Baseline F1':<12} {'Forge F1':<12} {'Œî':>8}")
        self.log("-" * 80)
        
        for exp in self.results:
            if exp.task_type in ['classification', 'text_classification']:
                b_metric = exp.baseline_metrics.f1
                f_metric = exp.forge_metrics.f1
                delta = exp.improvement.get('f1', 0)
            else:
                b_metric = exp.baseline_metrics.r2
                f_metric = exp.forge_metrics.r2
                delta = exp.improvement.get('r2', 0)
            
            delta_str = f"+{delta:.1f}%" if delta > 0 else f"{delta:.1f}%"
            self.log(f"{exp.dataset_name:<35} {exp.task_type:<15} {b_metric:<12.4f} {f_metric:<12.4f} {delta_str:>8}")
    
    def to_dataframe(self) -> pd.DataFrame:
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ DataFrame"""
        rows = []
        for exp in self.results:
            row = {
                'dataset': exp.dataset_name,
                'task': exp.task_type,
            }
            for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'mse', 'mae', 'r2']:
                row[f'baseline_{metric}'] = getattr(exp.baseline_metrics, metric)
                row[f'forge_{metric}'] = getattr(exp.forge_metrics, metric)
                row[f'improvement_{metric}'] = exp.improvement.get(metric, 0)
            rows.append(row)
        
        return pd.DataFrame(rows)


# ==================== –ó–∞–ø—É—Å–∫ ====================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("="*60)
    print("   ML DATA FORGE - BENCHMARK PIPELINE")
    print("   Comparing baseline vs automated preprocessing")
    print("="*60)
    
    # –°–æ–∑–¥–∞—ë–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline = BenchmarkPipeline(verbose=True)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    datasets = [
        # –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        TabularClassificationDataset(
            n_samples=1000,
            n_features=10,
            missing_rate=0.1,
            imbalance_ratio=0.3
        ),
        
        # –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤
        TabularClassificationDataset(
            n_samples=800,
            n_features=8,
            missing_rate=0.25,
            imbalance_ratio=0.4
        ),
        
        # –†–µ–≥—Ä–µ—Å—Å–∏—è
        TabularRegressionDataset(
            n_samples=1000,
            n_features=8,
            missing_rate=0.15
        ),
        
        # –°–∏–ª—å–Ω–æ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        ImbalancedDataset(n_samples=2000),
        
        # –¢–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        TextClassificationDataset(
            n_samples=400,
            imbalance_ratio=0.25
        ),
    ]
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
    pipeline.run_all(datasets)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_df = pipeline.to_dataframe()
    results_df.to_csv('benchmark_results.csv', index=False)
    print("\n‚úÖ Results saved to benchmark_results.csv")
    
    return pipeline


if __name__ == "__main__":
    pipeline = main()