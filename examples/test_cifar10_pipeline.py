"""
–ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è automl_data –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ CIFAR-10.

–í–∫–ª—é—á–∞–µ—Ç:
1. –ó–∞–≥—Ä—É–∑–∫—É CIFAR-10 —á–µ—Ä–µ–∑ torchvision
2. –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å AutoForge
3. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É
4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (CNN) —Å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
5. –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç—á—ë—Ç—ã –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import sys
import warnings
import logging
from typing import Dict, Any, Optional, Tuple
import time

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)-25s | %(levelname)-8s | %(message)s',
    datefmt='%H:%M:%S'
)

def get_forge_result_stats(result):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ ForgeResult"""
    stats = {
        'original_size': 'N/A',
        'augmented_size': 'N/A',
        'total_size': 'N/A',
        'quality_score': 'N/A',
        'augmentation_info': {}
    }
    
    if not result:
        return stats
    
    # –°–ø–æ—Å–æ–± 1: –ß–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
    if hasattr(result, 'container') and result.container:
        container = result.container
        
        # –ò–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        if hasattr(container, 'metadata') and container.metadata:
            stats['augmentation_info'] = container.metadata.get('augmentation', {})
            stats['original_size'] = container.metadata.get('augmentation', {}).get('original_size', 'N/A')
            stats['augmented_size'] = container.metadata.get('augmentation', {}).get('augmented_size', 'N/A')
        
        # –ò–∑ –¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        if hasattr(container, 'data') and container.data is not None:
            stats['total_size'] = len(container.data)
            
            if '_augmented' in container.data.columns:
                aug_count = container.data['_augmented'].sum()
                orig_count = len(container.data) - aug_count
                stats['original_size'] = orig_count
                stats['augmented_size'] = aug_count
    
    # –°–ø–æ—Å–æ–± 2: –ß–µ—Ä–µ–∑ ForgeResult –Ω–∞–ø—Ä—è–º—É—é
    if hasattr(result, 'data'):
        stats['total_size'] = len(result.data)
    
    if hasattr(result, 'quality_score'):
        stats['quality_score'] = result.quality_score
    
    return stats


def analyze_augmentation_results(df, result):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    print("\nüîç –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ò")
    print("-" * 40)
    
    if not result or not hasattr(result, 'data'):
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    original_size = len(df)
    final_size = len(result.data)
    increase = final_size / original_size if original_size > 0 else 0
    
    print(f"üìä –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
    print(f"   ‚Ä¢ –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {original_size}")
    print(f"   ‚Ä¢ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {final_size}")
    print(f"   ‚Ä¢ –£–≤–µ–ª–∏—á–µ–Ω–∏–µ: {increase:.2f}x")
    
    if hasattr(result, 'quality_score'):
        print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {result.quality_score:.1%}")
    
    # –ê–Ω–∞–ª–∏–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    if '_augmented' in result.data.columns:
        aug_count = result.data['_augmented'].sum()
        orig_count = final_size - aug_count
        
        print(f"\nüéØ –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–Ø:")
        print(f"   ‚Ä¢ –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {aug_count}")
        print(f"   ‚Ä¢ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö: {orig_count}")
        print(f"   ‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {aug_count/final_size*100:.1f}%" if final_size > 0 else "N/A")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º
        if 'class_id' in result.data.columns:
            print(f"\nüìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ö–õ–ê–°–°–ê–ú:")
            
            # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            original_dist = df['class_id'].value_counts().sort_index()
            
            # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            processed_dist = result.data['class_id'].value_counts().sort_index()
            
            # –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            augmented_dist = result.data[result.data['_augmented']]['class_id'].value_counts().sort_index()
            
            for class_id in sorted(original_dist.index):
                orig = original_dist.get(class_id, 0)
                proc = processed_dist.get(class_id, 0)
                aug = augmented_dist.get(class_id, 0)
                
                increase = proc / orig if orig > 0 else 0
                percentage = proc / final_size * 100 if final_size > 0 else 0
                
                print(f"   ‚Ä¢ –ö–ª–∞—Å—Å {class_id}:")
                print(f"     - –ë—ã–ª–æ: {orig}")
                print(f"     - –°—Ç–∞–ª–æ: {proc} ({increase:.2f}x)")
                print(f"     - –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ: {aug}")
                print(f"     - –í –¥–∞—Ç–∞—Å–µ—Ç–µ: {percentage:.1f}%")

print("=" * 70)
print("üöÄ –ü–ê–ô–ü–õ–ê–ô–ù –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ù–ê CIFAR-10")
print("=" * 70)

# ============================================
# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê CIFAR-10
# ============================================

def load_cifar10_as_dataframe(output_dir: Path = "cifar10_dataset", fix_test_size: int = 500) -> Tuple[pd.DataFrame, Path]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç CIFAR-10 –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ DataFrame.
    
    Args:
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        fix_test_size: –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞
        
    Returns:
        DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    """
    try:
        import torch
        import torchvision
        import torchvision.transforms as transforms
        from PIL import Image
    except ImportError:
        print("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å torch –∏ torchvision")
        print("   pip install torch torchvision")
        sys.exit(1)
    
    print("\nüì• –ó–ê–ì–†–£–ó–ö–ê CIFAR-10")
    print("-" * 40)
    
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # –ö–ª–∞—Å—Å—ã CIFAR-10
    classes = ('plane', 'car', 'bird', 'cat', 'deer', 
               'dog', 'frog', 'horse', 'ship', 'truck')
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    print("–ó–∞–≥—Ä—É–∂–∞—é CIFAR-10...")
    transform = transforms.Compose([
        transforms.ToTensor()
    ])
    
    trainset = torchvision.datasets.CIFAR10(
        root='./data', 
        train=True,
        download=True, 
        transform=transform
    )
    
    testset = torchvision.datasets.CIFAR10(
        root='./data',
        train=False,
        download=True,
        transform=transform
    )
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ:")
    print(f"   ‚Ä¢ Train: {len(trainset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"   ‚Ä¢ Test: {len(testset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ: {len(trainset) + len(testset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"   ‚Ä¢ –ö–ª–∞—Å—Å—ã: {len(classes)}")
    
    # –°–æ–∑–¥–∞—ë–º DataFrame
    data = []
    image_counter = 0
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    def save_image(tensor, filename):
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º tensor –≤ numpy array
        img_np = tensor.numpy().transpose(1, 2, 0) * 255
        img_np = img_np.astype(np.uint8)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ PNG
        img = Image.fromarray(img_np)
        img.save(filename)
    
    print("\nüìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º train set (–≤–æ–∑—å–º—ë–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
    n_train_samples = 2500  # 2500 –¥–ª—è train
    n_test_samples = fix_test_size  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä test
    
    print(f"–ë–µ—Ä—É –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ:")
    print(f"   ‚Ä¢ Train samples: {n_train_samples}")
    print(f"   ‚Ä¢ Test samples: {n_test_samples} (—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ)")
    
    # Train images - –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ
    train_indices = np.random.choice(len(trainset), n_train_samples, replace=False)
    for idx in train_indices:
        img, label = trainset[idx]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img_path = output_dir / f"train_{image_counter:06d}.png"
        save_image(img, img_path)
        
        data.append({
            "image_id": f"train_{image_counter:06d}",
            "image_path": img_path.name,
            "label": classes[label],
            "class_id": int(label),
            "dataset": "train"
        })
        
        image_counter += 1
    
    # Test images - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤—ã–±–∏—Ä–∞–µ–º –í–°–ï 500 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –∫–ª–∞—Å—Å–∞–º
    print(f"\nüìä –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ ({n_test_samples} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π):")
    
    # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ –∫–ª–∞—Å—Å–∞–º
    test_indices_by_class = {}
    for idx, (_, label) in enumerate(testset):
        class_id = label
        if class_id not in test_indices_by_class:
            test_indices_by_class[class_id] = []
        test_indices_by_class[class_id].append(idx)
    
    # –ë–µ—Ä–µ–º —Ä–∞–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    per_class_count = n_test_samples // len(classes)
    extra_count = n_test_samples % len(classes)
    
    test_indices = []
    for class_id in range(len(classes)):
        class_indices = test_indices_by_class.get(class_id, [])
        if len(class_indices) >= per_class_count:
            selected = np.random.choice(class_indices, per_class_count, replace=False)
            test_indices.extend(selected)
        else:
            # –ï—Å–ª–∏ –≤ –∫–ª–∞—Å—Å–µ –º–µ–Ω—å—à–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –±–µ—Ä–µ–º –≤—Å–µ
            test_indices.extend(class_indices)
    
    # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    if extra_count > 0:
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∏–Ω–¥–µ–∫—Å—ã
        all_test_indices = list(range(len(testset)))
        remaining_indices = [idx for idx in all_test_indices if idx not in test_indices]
        if len(remaining_indices) >= extra_count:
            extra_selected = np.random.choice(remaining_indices, extra_count, replace=False)
            test_indices.extend(extra_selected)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    test_indices = test_indices[:n_test_samples]
    
    print(f"   ‚Ä¢ –û—Ç–æ–±—Ä–∞–Ω–æ {len(test_indices)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    for idx in test_indices:
        img, label = testset[idx]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img_path = output_dir / f"test_{image_counter:06d}.png"
        save_image(img, img_path)
        
        data.append({
            "image_id": f"test_{image_counter:06d}",
            "image_path": img_path.name,
            "label": classes[label],
            "class_id": int(label),
            "dataset": "test"
        })
        
        image_counter += 1
    
    # –°–æ–∑–¥–∞—ë–º DataFrame
    df = pd.DataFrame(data)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata_path = output_dir / "cifar10_metadata.csv"
    df.to_csv(metadata_path, index=False)
    
    print(f"\n‚úÖ –î–∞—Ç—ã—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω:")
    print(f"   ‚Ä¢ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(df)}")
    print(f"   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {output_dir}")
    print(f"   ‚Ä¢ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata_path}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:")
    print(f"   ‚Ä¢ Train set: {len(df[df['dataset'] == 'train'])}")
    print(f"   ‚Ä¢ Test set: {len(df[df['dataset'] == 'test'])}")
    
    class_counts = df['class_id'].value_counts().sort_index()
    for class_id, count in class_counts.items():
        percentage = count / len(df) * 100
        class_name = classes[class_id]
        print(f"   ‚Ä¢ {class_name} ({class_id}): {count} ({percentage:.1f}%)")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ test set
    test_df = df[df['dataset'] == 'test']
    if len(test_df) > 0:
        print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –í TEST SET:")
        test_class_counts = test_df['class_id'].value_counts().sort_index()
        for class_id, count in test_class_counts.items():
            percentage = count / len(test_df) * 100
            class_name = classes[class_id]
            print(f"   ‚Ä¢ {class_name} ({class_id}): {count} ({percentage:.1f}%)")
    
    return df, output_dir


# ============================================
# 2. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–¢–ê–°–ï–¢–ê
# ============================================

def visualize_cifar10_dataset(df: pd.DataFrame, output_dir: Path):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–∑—Ü–æ–≤ CIFAR-10"""
    print("\nüëÄ –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø CIFAR-10")
    print("-" * 40)
    
    try:
        from automl_data import DataContainer
        from PIL import Image
        import matplotlib.pyplot as plt
        
        # –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        container = DataContainer(
            data=df.copy(),
            target_column="class_id",
            image_column="image_path",
            image_dir=output_dir
        )
        
        print("\nüìä –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•:")
        print(f"   ‚Ä¢ –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {container.data_type.name}")
        print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä: {container.shape}")
        print(f"   ‚Ä¢ –ö–æ–ª–æ–Ω–∫–∏: {len(container.columns)}")
        print(f"   ‚Ä¢ –ß–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {len(container.numeric_columns)}")
        print(f"   ‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {len(container.categorical_columns)}")
        
        if container.class_distribution:
            print(f"\nüìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:")
            for class_name, count in container.class_distribution.items():
                percentage = count / len(container) * 100
                print(f"   ‚Ä¢ –ö–ª–∞—Å—Å {class_name}: {count} ({percentage:.1f}%)")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–∑—Ü–æ–≤
        print("\nüñºÔ∏è –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –û–ë–†–ê–ó–¶–û–í:")
        
        # –°–æ–∑–¥–∞—ë–º grid –∏–∑ –ø—Ä–∏–º–µ—Ä–æ–≤
        fig, axes = plt.subplots(2, 5, figsize=(15, 6))
        axes = axes.flatten()
        
        # –ë–µ—Ä–µ–º –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–º–µ—Ä—É –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        classes = df['class_id'].unique()
        
        for i, class_id in enumerate(sorted(classes)[:10]):  # –º–∞–∫—Å–∏–º—É–º 10 –∫–ª–∞—Å—Å–æ–≤
            class_samples = df[df['class_id'] == class_id]
            if len(class_samples) > 0:
                sample = class_samples.iloc[0]
                img_path = output_dir / sample['image_path']
                
                try:
                    img = Image.open(img_path)
                    axes[i].imshow(img)
                    axes[i].set_title(f"{sample['label']} (ID: {class_id})")
                    axes[i].axis('off')
                except Exception as e:
                    axes[i].text(0.5, 0.5, f"Error: {e}", 
                               ha='center', va='center')
                    axes[i].axis('off')
        
        plt.suptitle("–ü—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π CIFAR-10", fontsize=16)
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
        viz_path = output_dir / "cifar10_samples.png"
        plt.savefig(viz_path, dpi=150, bbox_inches='tight')
        print(f"   ‚Ä¢ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {viz_path}")
        
        plt.show()
        
        return container
        
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        return None


# ============================================
# 3. –ü–û–õ–ù–´–ô –¶–ò–ö–õ AUTOFORGE –ù–ê CIFAR-10
# ============================================

def test_cifar10_with_autoforge(df: pd.DataFrame, output_dir: Path) -> Dict[str, Any]:
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ CIFAR-10 —Å AutoForge.
    """
    print("\n" + "=" * 60)
    print("‚öôÔ∏è –ü–û–õ–ù–´–ô –¶–ò–ö–õ AUTOFORGE –ù–ê CIFAR-10")
    print("=" * 60)
    
    try:
        from automl_data import AutoForge
        from automl_data.core.config import ImageConfig
        
        # –®–∞–≥ 1: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        print("\n1Ô∏è‚É£ –®–ê–ì: –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø")
        print("-" * 30)
        
        image_config = ImageConfig(
            # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
            target_size=(32, 32),  # CIFAR-10 —Ä–∞–∑–º–µ—Ä
            normalize=True,
            keep_aspect_ratio=False,
            
            # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è - –í–ö–õ–Æ–ß–ê–ï–ú –¥–ª—è train
            augment=True,
            augment_factor=3.0,  # –£–≤–µ–ª–∏—á–∏—Ç—å –≤ 3 —Ä–∞–∑–∞
            
            # –ú–µ—Ç–æ–¥—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            horizontal_flip=True,
            rotation_range=10,
            brightness_range=(0.8, 1.2),
            contrast_range=(0.8, 1.2),
            zoom_range=(0.9, 1.1),
            
            # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ - –û–¢–ö–õ–Æ–ß–ê–ï–ú –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è
            balance_classes=False,
        )
        
        print(f"   ‚Ä¢ ImageConfig —Å–æ–∑–¥–∞–Ω –¥–ª—è CIFAR-10")
        print(f"   ‚Ä¢ Target size: {image_config.target_size}")
        print(f"   ‚Ä¢ Augment: {image_config.augment}")
        print(f"   ‚Ä¢ Balance classes: {image_config.balance_classes}")
        print(f"   ‚Ä¢ Augment factor: {image_config.augment_factor}")
        
        # –®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ AutoForge
        print("\n2Ô∏è‚É£ –®–ê–ì: –°–û–ó–î–ê–ù–ò–ï AUTOFORGE")
        print("-" * 30)
        
        forge = AutoForge(
            target="class_id",
            image_column="image_path",
            image_dir=output_dir,
            task="classification",
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            image_config=image_config,
            
            # –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            balance=False,
            
            # –†–∞–∑–±–∏–µ–Ω–∏–µ
            test_size=0.2,
            stratify=True,
            random_state=42,
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            verbose=True
        )
        
        print(f"   ‚Ä¢ AutoForge —Å–æ–∑–¥–∞–Ω")
        print(f"   ‚Ä¢ Target: {forge.config.target}")
        print(f"   ‚Ä¢ Task: {forge.config.task.value}")
        
        # –®–∞–≥ 3: Fit
        print("\n3Ô∏è‚É£ –®–ê–ì: FIT (–ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•)")
        print("-" * 30)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train –∏ test
        train_df = df[df['dataset'] == 'train'].copy()
        test_df = df[df['dataset'] == 'test'].copy()
        
        print(f"   ‚Ä¢ Train size: {len(train_df)}")
        print(f"   ‚Ä¢ Test size: {len(test_df)} (—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ test
        print(f"\n   üìä TEST SET –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï:")
        test_counts = test_df['class_id'].value_counts().sort_index()
        for class_id, count in test_counts.items():
            percentage = count / len(test_df) * 100
            print(f"     ‚Ä¢ –ö–ª–∞—Å—Å {class_id}: {count} ({percentage:.1f}%)")
        
        start_time = time.time()
        forge.fit(train_df)
        fit_time = time.time() - start_time
        
        print(f"   ‚Ä¢ Pipeline –ø–æ—Å—Ç—Ä–æ–µ–Ω")
        print(f"   ‚Ä¢ –®–∞–≥–æ–≤ –≤ pipeline: {len(forge._pipeline) if forge._pipeline else 0}")
        print(f"   ‚Ä¢ –í—Ä–µ–º—è fit: {fit_time:.2f} —Å–µ–∫")
        
        # –®–∞–≥ 4: Transform train –¥–∞–Ω–Ω—ã—Ö
        print("\n4Ô∏è‚É£ –®–ê–ì: TRANSFORM TRAIN –î–ê–ù–ù–´–•")
        print("-" * 30)
        
        start_time = time.time()
        train_result = forge.transform(train_df)
        transform_time = time.time() - start_time
        
        print(f"   ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        print(f"   ‚Ä¢ –í—Ä–µ–º—è transform: {transform_time:.2f} —Å–µ–∫")
        print(f"   ‚Ä¢ –®–∞–≥–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {len(train_result.steps)}")
        
        # –®–∞–≥ 5: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n5Ô∏è‚É£ –®–ê–ì: –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print("-" * 30)
        
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–†–ê–ë–û–¢–ö–ò:")
        print(f"   ‚Ä¢ –ò—Å—Ö–æ–¥–Ω—ã–π train —Ä–∞–∑–º–µ—Ä: {len(train_df)}")
        print(f"   ‚Ä¢ –§–∏–Ω–∞–ª—å–Ω—ã–π train —Ä–∞–∑–º–µ—Ä: {len(train_result.data)}")
        print(f"   ‚Ä¢ –£–≤–µ–ª–∏—á–µ–Ω–∏–µ: {len(train_result.data)/len(train_df):.2f}x")
        print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {train_result.quality_score:.1%}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        if "_augmented" in train_result.data.columns:
            aug_count = train_result.data["_augmented"].sum()
            print(f"\nüîç –ü–†–û–í–ï–†–ö–ê –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ò:")
            print(f"   ‚Ä¢ –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {aug_count}")
            print(f"   ‚Ä¢ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö: {len(train_result.data) - aug_count}")
            print(f"   ‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {aug_count/len(train_result.data)*100:.1f}%")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º –ø–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        if train_result.container.class_distribution:
            print(f"\nüìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ö–õ–ê–°–°–ê–ú –ü–û–°–õ–ï –û–ë–†–ê–ë–û–¢–ö–ò:")
            counts = list(train_result.container.class_distribution.values())
            
            for class_name, count in train_result.container.class_distribution.items():
                original_count = len(train_df[train_df['class_id'] == int(class_name)])
                increase = count / original_count if original_count > 0 else 0
                percentage = count / len(train_result.data) * 100
                print(f"   ‚Ä¢ –ö–ª–∞—Å—Å {class_name}: –±—ã–ª–æ {original_count}, —Å—Ç–∞–ª–æ {count} ({increase:.2f}x, {percentage:.1f}%)")
            
            if len(counts) >= 2:
                ratio = min(counts) / max(counts)
                print(f"   ‚Ä¢ –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ min/max: {ratio:.2f}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–ª–∏—Ç—ã
        X_train, X_val, y_train, y_val = train_result.get_splits(
            test_size=0.2,
            random_state=42,
            stratify=True
        )
        
        print(f"\nüéØ –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/VAL:")
        print(f"   ‚Ä¢ X_train: {X_train.shape}")
        print(f"   ‚Ä¢ X_val: {X_val.shape}")
        print(f"   ‚Ä¢ y_train: {y_train.shape}")
        print(f"   ‚Ä¢ y_val: {y_val.shape}")
        
        # –®–∞–≥ 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ë–ï–ó –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        print("\n6Ô∏è‚É£ –®–ê–ì: –û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–• (–ë–ï–ó –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ò)")
        print("-" * 30)
        
        # –í–ê–ñ–ù–û: –û—Ç–∫–ª—é—á–∞–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è test
        test_image_config = ImageConfig(
            target_size=(32, 32),
            normalize=True,
            keep_aspect_ratio=False,
            augment=False,  # ‚Üê –û–¢–ö–õ–Æ–ß–ê–ï–ú –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é!
            balance_classes=False,
        )
        
        # –°–æ–∑–¥–∞—ë–º –æ—Ç–¥–µ–ª—å–Ω—ã–π forge –¥–ª—è test (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
        test_forge = AutoForge(
            target="class_id",
            image_column="image_path",
            image_dir=output_dir,
            task="classification",
            image_config=test_image_config,
            balance=False,
            test_size=0.0,  # –ù–µ —Ä–∞–∑–±–∏–≤–∞–µ–º test
            verbose=False  # –£–º–µ–Ω—å—à–∞–µ–º –≤—ã–≤–æ–¥
        )
        
        # Fit –Ω–∞ —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        test_forge.fit(train_df)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º train –¥–ª—è fit, –Ω–æ transform –ø—Ä–∏–º–µ–Ω—è–µ–º –∫ test
        
        # Transform test –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
        test_result = test_forge.transform(test_df)
        
        print(f"   ‚Ä¢ Test size –¥–æ: {len(test_df)}")
        print(f"   ‚Ä¢ Test size –ø–æ—Å–ª–µ: {len(test_result.data)} (–¥–æ–ª–∂–Ω–æ –æ—Å—Ç–∞—Ç—å—Å—è {len(test_df)})")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ test –¥–∞–Ω–Ω—ã–µ –Ω–µ –±—ã–ª–∏ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã
        if "_augmented" in test_result.data.columns:
            test_aug_count = test_result.data["_augmented"].sum()
            print(f"   ‚Ä¢ –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤ test: {test_aug_count} (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 0)")
            
            if test_aug_count > 0:
                print(f"   ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: Test –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã!")
            else:
                print(f"   ‚úÖ Test –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
        else:
            print(f"   ‚úÖ Test –¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ test
        if test_result.container.class_distribution:
            print(f"\n   üìä TEST SET –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û–°–õ–ï –û–ë–†–ê–ë–û–¢–ö–ò:")
            test_counts = list(test_result.container.class_distribution.values())
            
            for class_name, count in test_result.container.class_distribution.items():
                percentage = count / len(test_result.data) * 100
                print(f"     ‚Ä¢ –ö–ª–∞—Å—Å {class_name}: {count} ({percentage:.1f}%)")
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if train_result:
            analyze_augmentation_results(train_df, train_result)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = {
            'forge': forge,
            'train_result': train_result,
            'test_result': test_result,
            'X_train': X_train,
            'X_val': X_val,
            'y_train': y_train,
            'y_val': y_val,
            'X_test': test_result.data,
            'y_test': test_result.data['class_id'] if 'class_id' in test_result.data.columns else None,
            'fit_time': fit_time,
            'transform_time': transform_time,
            'total_time': fit_time + transform_time,
            'original_train_df': train_df,
            'original_test_df': test_df,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π test
            'output_dir': output_dir
        }
        
        return results
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ AutoForge: {e}")
        import traceback
        traceback.print_exc()
        return None


# ============================================
# 4. –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ù–ê CIFAR-10
# ============================================

def train_cifar10_models(results: Dict[str, Any]) -> Dict[str, Any]:
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö CIFAR-10.
    
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç:
    1. –ú–æ–¥–µ–ª—å –Ω–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö (train/val/test)
    2. –ú–æ–¥–µ–ª—å –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (train/val/test)
    """
    print("\n" + "=" * 60)
    print("üß† –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ù–ê CIFAR-10")
    print("=" * 60)
    
    try:
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import Dataset, DataLoader
        from torchvision import transforms
        import cv2
        
        # –ö–ª–∞—Å—Å –¥–∞—Ç–∞—Å–µ—Ç–∞
        class CIFAR10Dataset(Dataset):
            def __init__(self, df, image_dir, transform=None):
                self.df = df.reset_index(drop=True)
                self.image_dir = Path(image_dir)
                self.transform = transform
            
            def __len__(self):
                return len(self.df)
            
            def __getitem__(self, idx):
                row = self.df.iloc[idx]
                img_path = self.image_dir / row["image_path"]
                
                try:
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    img = cv2.imread(str(img_path))
                    if img is None:
                        img = np.zeros((32, 32, 3), dtype=np.uint8)
                    
                    # –†–µ—Å–∞–π–∑ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if img.shape[:2] != (32, 32):
                        img = cv2.resize(img, (32, 32))
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR -> RGB
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è [0, 1]
                    img = img.astype(np.float32) / 255.0
                    
                    # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –¥–ª—è PyTorch (H, W, C) -> (C, H, W)
                    img = np.transpose(img, (2, 0, 1))
                    
                    # –í —Ç–µ–Ω–∑–æ—Ä
                    img = torch.FloatTensor(img)
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
                    if self.transform:
                        img = self.transform(img)
                    
                    # –ú–µ—Ç–∫–∞
                    label = int(row["class_id"])
                    
                    return img, label
                    
                except Exception:
                    # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    img = torch.zeros((3, 32, 32))
                    label = 0
                    return img, label
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ CNN –¥–ª—è CIFAR-10
        class CIFAR10CNN(nn.Module):
            def __init__(self, num_classes=10):
                super().__init__()
                self.features = nn.Sequential(
                    nn.Conv2d(3, 32, 3, padding=1),
                    nn.BatchNorm2d(32),
                    nn.ReLU(),
                    nn.Conv2d(32, 32, 3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                    nn.Dropout(0.25),
                    
                    nn.Conv2d(32, 64, 3, padding=1),
                    nn.BatchNorm2d(64),
                    nn.ReLU(),
                    nn.Conv2d(64, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                    nn.Dropout(0.25),
                    
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(),
                    nn.Conv2d(128, 128, 3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                    nn.Dropout(0.25),
                )
                
                self.classifier = nn.Sequential(
                    nn.Flatten(),
                    nn.Linear(128 * 4 * 4, 512),
                    nn.ReLU(),
                    nn.Dropout(0.5),
                    nn.Linear(512, num_classes)
                )
            
            def forward(self, x):
                x = self.features(x)
                x = self.classifier(x)
                return x
        
        # –§—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
        def train_epoch(model, dataloader, criterion, optimizer, device):
            model.train()
            running_loss = 0.0
            correct = 0
            total = 0
            
            for data, target in dataloader:
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
            
            accuracy = 100 * correct / total if total > 0 else 0
            return running_loss / len(dataloader), accuracy
        
        def validate(model, dataloader, criterion, device):
            model.eval()
            running_loss = 0.0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for data, target in dataloader:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    loss = criterion(output, target)
                    running_loss += loss.item()
                    
                    _, predicted = torch.max(output.data, 1)
                    total += target.size(0)
                    correct += (predicted == target).sum().item()
            
            accuracy = 100 * correct / total if total > 0 else 0
            return running_loss / len(dataloader), accuracy
        
        def test_model(model, dataloader, criterion, device):
            """–§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ test set"""
            model.eval()
            running_loss = 0.0
            correct = 0
            total = 0
            all_predictions = []
            all_targets = []
            
            with torch.no_grad():
                for data, target in dataloader:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    loss = criterion(output, target)
                    running_loss += loss.item()
                    
                    _, predicted = torch.max(output.data, 1)
                    total += target.size(0)
                    correct += (predicted == target).sum().item()
                    
                    all_predictions.extend(predicted.cpu().numpy())
                    all_targets.extend(target.cpu().numpy())
            
            accuracy = 100 * correct / total if total > 0 else 0
            
            # –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
            from sklearn.metrics import classification_report, confusion_matrix
            report = classification_report(all_targets, all_predictions, output_dict=True)
            cm = confusion_matrix(all_targets, all_predictions)
            
            return running_loss / len(dataloader), accuracy, report, cm
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"   ‚Ä¢ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        print(f"   ‚Ä¢ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("\nA. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
        print("-" * 30)
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        train_transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.RandomHorizontalFlip(p=0.3),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        
        test_transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        from sklearn.model_selection import train_test_split
        
        print("\nB. –ú–û–î–ï–õ–¨ –ù–ê –°–´–†–´–• –î–ê–ù–ù–´–•")
        print("-" * 30)
        
        # –ë–µ—Ä—ë–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        original_train_df = results.get('original_train_df')
        original_test_df = results.get('original_test_df')  # ‚Üê –î–û–ë–ê–í–ò–õ–ò test –¥–∞–Ω–Ω—ã–µ
        
        if original_train_df is None:
            print("   ‚ö†Ô∏è –ù–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
            print("   –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å—ã—Ä—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
            raw_results = None
        else:
            # –†–∞–∑–¥–µ–ª—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/val
            raw_train, raw_val = train_test_split(
                original_train_df, 
                test_size=0.2, 
                random_state=42, 
                stratify=original_train_df['class_id']
            )
            
            # –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç—ã
            raw_train_dataset = CIFAR10Dataset(raw_train, results['output_dir'], transform=train_transform)
            raw_val_dataset = CIFAR10Dataset(raw_val, results['output_dir'], transform=test_transform)
            
            # Test –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–• test –¥–∞–Ω–Ω—ã—Ö
            raw_test_dataset = CIFAR10Dataset(original_test_df, results['output_dir'], transform=test_transform)
            
            raw_train_loader = DataLoader(raw_train_dataset, batch_size=64, shuffle=True)
            raw_val_loader = DataLoader(raw_val_dataset, batch_size=64, shuffle=False)
            raw_test_loader = DataLoader(raw_test_dataset, batch_size=64, shuffle=False)
            
            # –°–æ–∑–¥–∞—ë–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            raw_model = CIFAR10CNN(num_classes=10).to(device)
            raw_criterion = nn.CrossEntropyLoss()
            raw_optimizer = optim.Adam(raw_model.parameters(), lr=0.001, weight_decay=1e-4)
            raw_scheduler = optim.lr_scheduler.StepLR(raw_optimizer, step_size=10, gamma=0.5)
            
            print(f"   ‚Ä¢ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in raw_model.parameters()):,}")
            print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä train: {len(raw_train)}")
            print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä val: {len(raw_val)}")
            print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä test: {len(original_test_df)} (–æ—Ç–¥–µ–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä)")
            
            # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ (5 —ç–ø–æ—Ö –¥–ª—è —Ç–µ—Å—Ç–∞)
            raw_train_losses = []
            raw_val_losses = []
            raw_val_accs = []
            
            for epoch in range(5):
                train_loss, train_acc = train_epoch(raw_model, raw_train_loader, raw_criterion, raw_optimizer, device)
                val_loss, val_acc = validate(raw_model, raw_val_loader, raw_criterion, device)
                raw_scheduler.step()
                
                raw_train_losses.append(train_loss)
                raw_val_losses.append(val_loss)
                raw_val_accs.append(val_acc)
                
                print(f"   Epoch {epoch+1}/5: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            
            # –§–ò–ù–ê–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –Ω–∞ test set
            print(f"\n   üìä –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê TEST SET:")
            raw_test_loss, raw_test_acc, raw_test_report, raw_test_cm = test_model(
                raw_model, raw_test_loader, raw_criterion, device
            )
            
            print(f"   ‚Ä¢ Test Loss: {raw_test_loss:.4f}, Test Acc: {raw_test_acc:.2f}%")
            
            raw_results = {
                'final_val_loss': raw_val_losses[-1],
                'final_val_acc': raw_val_accs[-1],
                'final_test_loss': raw_test_loss,
                'final_test_acc': raw_test_acc,
                'test_report': raw_test_report,
                'train_losses': raw_train_losses,
                'val_losses': raw_val_losses,
                'val_accs': raw_val_accs
            }
        
        print("\nC. –ú–û–î–ï–õ–¨ –ù–ê –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –î–ê–ù–ù–´–•")
        print("-" * 30)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ AutoForge
        X_train = results['X_train']
        X_val = results['X_val']
        y_train = results['y_train']
        y_val = results['y_val']
        X_test = results['X_test']  # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ test –¥–∞–Ω–Ω—ã–µ
        y_test = results['y_test']   # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ test –º–µ—Ç–∫–∏
        
        # –°–æ–∑–¥–∞—ë–º DataFrame –∏–∑ —Å–ø–ª–∏—Ç–æ–≤
        train_proc = pd.concat([X_train, y_train], axis=1)
        val_proc = pd.concat([X_val, y_val], axis=1)
        test_proc = X_test.copy()
        if y_test is not None:
            test_proc['class_id'] = y_test
        
        # –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç—ã
        proc_train_dataset = CIFAR10Dataset(train_proc, results['output_dir'], transform=train_transform)
        proc_val_dataset = CIFAR10Dataset(val_proc, results['output_dir'], transform=test_transform)
        proc_test_dataset = CIFAR10Dataset(test_proc, results['output_dir'], transform=test_transform)
        
        proc_train_loader = DataLoader(proc_train_dataset, batch_size=64, shuffle=True)
        proc_val_loader = DataLoader(proc_val_dataset, batch_size=64, shuffle=False)
        proc_test_loader = DataLoader(proc_test_dataset, batch_size=64, shuffle=False)
        
        # –°–æ–∑–¥–∞—ë–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        proc_model = CIFAR10CNN(num_classes=10).to(device)
        proc_criterion = nn.CrossEntropyLoss()
        proc_optimizer = optim.Adam(proc_model.parameters(), lr=0.001, weight_decay=1e-4)
        proc_scheduler = optim.lr_scheduler.StepLR(proc_optimizer, step_size=10, gamma=0.5)
        
        print(f"   ‚Ä¢ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in proc_model.parameters()):,}")
        print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä train (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π): {len(train_proc)}")
        print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä val (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π): {len(val_proc)}")
        print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä test (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π): {len(test_proc)}")
        print(f"   ‚Ä¢ –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(train_proc)/len(original_train_df):.2f}x")
        
        # –û–±—É—á–µ–Ω–∏–µ
        proc_train_losses = []
        proc_val_losses = []
        proc_val_accs = []
        
        for epoch in range(5):
            train_loss, train_acc = train_epoch(proc_model, proc_train_loader, proc_criterion, proc_optimizer, device)
            val_loss, val_acc = validate(proc_model, proc_val_loader, proc_criterion, device)
            proc_scheduler.step()
            
            proc_train_losses.append(train_loss)
            proc_val_losses.append(val_loss)
            proc_val_accs.append(val_acc)
            
            print(f"   Epoch {epoch+1}/5: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        
        # –§–ò–ù–ê–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –Ω–∞ test set
        print(f"\n   üìä –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê TEST SET:")
        proc_test_loss, proc_test_acc, proc_test_report, proc_test_cm = test_model(
            proc_model, proc_test_loader, proc_criterion, device
        )
        
        print(f"   ‚Ä¢ Test Loss: {proc_test_loss:.4f}, Test Acc: {proc_test_acc:.2f}%")
        
        proc_results = {
            'final_val_loss': proc_val_losses[-1],
            'final_val_acc': proc_val_accs[-1],
            'final_test_loss': proc_test_loss,
            'final_test_acc': proc_test_acc,
            'test_report': proc_test_report,
            'train_losses': proc_train_losses,
            'val_losses': proc_val_losses,
            'val_accs': proc_val_accs
        }
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print("-" * 30)
        
        if raw_results:
            print(f"   –°–´–†–´–ï –î–ê–ù–ù–´–ï:")
            print(f"   ‚Ä¢ Val Loss: {raw_results['final_val_loss']:.4f}, Val Acc: {raw_results['final_val_acc']:.2f}%")
            print(f"   ‚Ä¢ Test Loss: {raw_results['final_test_loss']:.4f}, Test Acc: {raw_results['final_test_acc']:.2f}%")
            
            print(f"\n   –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï:")
            print(f"   ‚Ä¢ Val Loss: {proc_results['final_val_loss']:.4f}, Val Acc: {proc_results['final_val_acc']:.2f}%")
            print(f"   ‚Ä¢ Test Loss: {proc_results['final_test_loss']:.4f}, Test Acc: {proc_results['final_test_acc']:.2f}%")
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ TEST SET
            if raw_results['final_test_loss'] > 0:
                loss_improvement = ((raw_results['final_test_loss'] - proc_results['final_test_loss']) / raw_results['final_test_loss'] * 100)
            else:
                loss_improvement = 0
            
            acc_improvement = proc_results['final_test_acc'] - raw_results['final_test_acc']
            
            print(f"\n   –£–õ–£–ß–®–ï–ù–ò–ï –ù–ê TEST SET:")
            print(f"   ‚Ä¢ –£–ª—É—á—à–µ–Ω–∏–µ loss: {loss_improvement:+.1f}%")
            print(f"   ‚Ä¢ –£–ª—É—á—à–µ–Ω–∏–µ accuracy: {acc_improvement:+.2f}%")
            
            if acc_improvement > 2.0:
                print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
            elif acc_improvement > 0.5:
                print(f"   ‚ö†Ô∏è  –ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
            elif acc_improvement > 0:
                print(f"   ‚ö†Ô∏è  –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
            else:
                print(f"   ‚ùå –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–ª—É—á—à–∏–ª–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        else:
            print(f"   ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - Test Loss: {proc_results['final_test_loss']:.4f}, Test Acc: {proc_results['final_test_acc']:.2f}%")
            print(f"   ‚Ä¢ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å—ã—Ä—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if raw_results:
            fig, axes = plt.subplots(1, 3, figsize=(15, 4))
            
            # Loss comparison
            axes[0].plot(raw_results['val_losses'], label='–°—ã—Ä—ã–µ (val)', marker='o')
            axes[0].plot(proc_results['val_losses'], label='–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ (val)', marker='s')
            axes[0].axhline(y=raw_results['final_test_loss'], color='r', linestyle='--', label='–°—ã—Ä—ã–µ (test)')
            axes[0].axhline(y=proc_results['final_test_loss'], color='b', linestyle='--', label='–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ (test)')
            axes[0].set_xlabel('–≠–ø–æ—Ö–∞')
            axes[0].set_ylabel('Loss')
            axes[0].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Loss')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)
            
            # Accuracy comparison
            axes[1].plot(raw_results['val_accs'], label='–°—ã—Ä—ã–µ (val)', marker='o')
            axes[1].plot(proc_results['val_accs'], label='–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ (val)', marker='s')
            axes[1].axhline(y=raw_results['final_test_acc'], color='r', linestyle='--', label='–°—ã—Ä—ã–µ (test)')
            axes[1].axhline(y=proc_results['final_test_acc'], color='b', linestyle='--', label='–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ (test)')
            axes[1].set_xlabel('–≠–ø–æ—Ö–∞')
            axes[1].set_ylabel('Accuracy (%)')
            axes[1].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Accuracy')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
            
            # Bar chart comparison
            comparison_data = {
                '–°—ã—Ä—ã–µ': [raw_results['final_val_loss'], raw_results['final_val_acc'], 
                         raw_results['final_test_loss'], raw_results['final_test_acc']],
                '–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ': [proc_results['final_val_loss'], proc_results['final_val_acc'],
                                proc_results['final_test_loss'], proc_results['final_test_acc']]
            }
            
            x = np.arange(4)
            width = 0.35
            
            axes[2].bar(x - width/2, [raw_results['final_val_loss'], raw_results['final_val_acc'], 
                                     raw_results['final_test_loss'], raw_results['final_test_acc']], 
                       width, label='–°—ã—Ä—ã–µ', color=['red', 'orange', 'darkred', 'darkorange'])
            axes[2].bar(x + width/2, [proc_results['final_val_loss'], proc_results['final_val_acc'],
                                     proc_results['final_test_loss'], proc_results['final_test_acc']], 
                       width, label='–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ', color=['blue', 'green', 'darkblue', 'darkgreen'])
            
            axes[2].set_xlabel('–ú–µ—Ç—Ä–∏–∫–∏')
            axes[2].set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
            axes[2].set_title('–§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (Val/Test)')
            axes[2].set_xticks(x)
            axes[2].set_xticklabels(['Val Loss', 'Val Acc', 'Test Loss', 'Test Acc'])
            axes[2].legend()
            axes[2].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig("cifar10_comparison_detailed.png", dpi=150, bbox_inches='tight')
            print(f"   ‚Ä¢ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: cifar10_comparison_detailed.png")
            plt.show()
        
        return {
            'raw_results': raw_results,
            'proc_results': proc_results,
            'device': str(device)
        }
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π: {e}")
        import traceback
        traceback.print_exc()
        return None


# ============================================
# 5. –°–û–ó–î–ê–ù–ò–ï –û–¢–ß–Å–¢–ê
# ============================================
def create_cifar10_report(df: pd.DataFrame, results: Dict[str, Any], model_results: Dict[str, Any]) -> str:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –ø–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞ CIFAR-10.
    """
    print("\n" + "=" * 60)
    print("üìÑ –°–û–ó–î–ê–ù–ò–ï –û–¢–ß–Å–¢–ê –ü–û CIFAR-10")
    print("=" * 60)
    
    report_lines = []
    
    report_lines.append("=" * 70)
    report_lines.append("–û–¢–ß–Å–¢ –ü–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Æ AUTOFORGE –ù–ê CIFAR-10")
    report_lines.append("=" * 70)
    report_lines.append(f"–î–∞—Ç–∞: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("")
    
    # 1. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
    report_lines.append("1. –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–¢–ê–°–ï–¢–ï CIFAR-10")
    report_lines.append("-" * 40)
    report_lines.append(f"‚Ä¢ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {len(df)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    report_lines.append(f"‚Ä¢ –†–∞–∑–º–µ—Ä train: {len(df[df['dataset'] == 'train'])}")
    report_lines.append(f"‚Ä¢ –†–∞–∑–º–µ—Ä test: {len(df[df['dataset'] == 'test'])}")
    report_lines.append(f"‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {df['class_id'].nunique()}")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    class_counts = df['class_id'].value_counts().sort_index()
    report_lines.append(f"‚Ä¢ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    for class_id, count in class_counts.items():
        percentage = count / len(df) * 100
        report_lines.append(f"  - –ö–ª–∞—Å—Å {class_id}: {count} ({percentage:.1f}%)")
    
    # 2. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã AutoForge
    if results:
        report_lines.append("\n2. –†–ï–ó–£–õ–¨–¢–ê–¢–´ AUTOFORGE")
        report_lines.append("-" * 40)
        report_lines.append(f"‚Ä¢ –í—Ä–µ–º—è fit: {results.get('fit_time', 0):.2f} —Å–µ–∫")
        report_lines.append(f"‚Ä¢ –í—Ä–µ–º—è transform: {results.get('transform_time', 0):.2f} —Å–µ–∫")
        report_lines.append(f"‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {results.get('total_time', 0):.2f} —Å–µ–∫")
        
        if results.get('train_result'):
            train_result = results['train_result']
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
            original_size = 'N/A'
            augmented_size = 'N/A'
            
            # –°–ø–æ—Å–æ–± 1: –ß–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
            if hasattr(train_result, 'container') and train_result.container:
                container = train_result.container
                
                # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                if hasattr(container, 'metadata') and container.metadata:
                    original_size = container.metadata.get('augmentation', {}).get('original_size', 'N/A')
                    augmented_size = container.metadata.get('augmentation', {}).get('augmented_size', 'N/A')
                
                # –ò–ª–∏ —á–µ—Ä–µ–∑ DataContainer –Ω–∞–ø—Ä—è–º—É—é
                if hasattr(container, 'data') and container.data is not None:
                    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
                    if '_augmented' in container.data.columns:
                        aug_count = container.data['_augmented'].sum()
                        orig_count = len(container.data) - aug_count
                        original_size = orig_count
                        augmented_size = aug_count
            
            # –°–ø–æ—Å–æ–± 2: –ß–µ—Ä–µ–∑ –∞—Ç—Ä–∏–±—É—Ç—ã ForgeResult
            elif hasattr(train_result, 'original_size'):
                original_size = train_result.original_size
            elif hasattr(train_result, 'input_size'):
                original_size = train_result.input_size
            
            # –í—ã—á–∏—Å–ª—è–µ–º —É–≤–µ–ª–∏—á–µ–Ω–∏–µ
            increase = 'N/A'
            if isinstance(original_size, (int, float)) and original_size > 0:
                total_size = len(train_result.data) if hasattr(train_result, 'data') else 'N/A'
                if isinstance(total_size, (int, float)):
                    increase = total_size / original_size
            
            report_lines.append(f"‚Ä¢ –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä train: {original_size}")
            report_lines.append(f"‚Ä¢ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä train: {len(train_result.data) if hasattr(train_result, 'data') else 'N/A'}")
            report_lines.append(f"‚Ä¢ –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {increase if isinstance(increase, str) else f'{increase:.2f}x'}")
            report_lines.append(f"‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {train_result.quality_score if hasattr(train_result, 'quality_score') else 'N/A'}")
            
            # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
            if hasattr(train_result, 'data') and '_augmented' in train_result.data.columns:
                aug_count = train_result.data["_augmented"].sum()
                report_lines.append(f"‚Ä¢ –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {aug_count}")
                report_lines.append(f"‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {aug_count/len(train_result.data)*100:.1f}%" if len(train_result.data) > 0 else "N/A")
    
    # 3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π
    if model_results:
        report_lines.append("\n3. –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô")
        report_lines.append("-" * 40)
        
        report_lines.append(f"‚Ä¢ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {model_results.get('device', 'N/A')}")
        
        if model_results.get('raw_results'):
            raw = model_results['raw_results']
            proc = model_results['proc_results']
            
            report_lines.append("\n   –°–´–†–´–ï –î–ê–ù–ù–´–ï:")
            report_lines.append(f"   ‚Ä¢ Final Validation Loss: {raw['final_val_loss']:.4f}")
            report_lines.append(f"   ‚Ä¢ Final Validation Accuracy: {raw['final_val_acc']:.2f}%")
            
            report_lines.append("\n   –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï:")
            report_lines.append(f"   ‚Ä¢ Final Validation Loss: {proc['final_val_loss']:.4f}")
            report_lines.append(f"   ‚Ä¢ Final Validation Accuracy: {proc['final_val_acc']:.2f}%")
            
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
            if raw['final_val_loss'] > 0:
                loss_improvement = ((raw['final_val_loss'] - proc['final_val_loss']) / raw['final_val_loss'] * 100)
            else:
                loss_improvement = 0
            
            acc_improvement = proc['final_val_acc'] - raw['final_val_acc']
            
            report_lines.append("\n   –°–†–ê–í–ù–ï–ù–ò–ï:")
            report_lines.append(f"   ‚Ä¢ –£–ª—É—á—à–µ–Ω–∏–µ Loss: {loss_improvement:+.1f}%")
            report_lines.append(f"   ‚Ä¢ –£–ª—É—á—à–µ–Ω–∏–µ Accuracy: {acc_improvement:+.2f}%")
            
            if acc_improvement > 2.0:
                report_lines.append(f"   ‚Ä¢ –í–´–í–û–î: ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏")
            elif acc_improvement > 0.5:
                report_lines.append(f"   ‚Ä¢ –í–´–í–û–î: ‚ö†Ô∏è  –ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏")
            elif acc_improvement > 0:
                report_lines.append(f"   ‚Ä¢ –í–´–í–û–î: ‚ö†Ô∏è  –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏")
            else:
                report_lines.append(f"   ‚Ä¢ –í–´–í–û–î: ‚ùå –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–ª—É—á—à–∏–ª–∞ —Ç–æ—á–Ω–æ—Å—Ç—å")
    
    # 4. –í—ã–≤–æ–¥—ã
    report_lines.append("\n4. –í–´–í–û–î–´")
    report_lines.append("-" * 40)
    
    if results and model_results:
        report_lines.append("‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ automl_data —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–ª–∞ CIFAR-10")
        report_lines.append("‚úÖ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        if model_results.get('proc_results', {}).get('final_val_acc', 0) > 50:
            report_lines.append("‚úÖ –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–ª–∞ —Ä–∞–∑—É–º–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ CIFAR-10")
        else:
            report_lines.append("‚ö†Ô∏è  –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∏–∂–µ –æ–∂–∏–¥–∞–µ–º–æ–π, —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞")
    else:
        report_lines.append("‚ùå –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
    
    # 5. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    report_lines.append("\n5. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    report_lines.append("-" * 40)
    report_lines.append("‚Ä¢ –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏")
    report_lines.append("‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É CNN")
    report_lines.append("‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –º–µ—Ç–æ–¥–æ–≤ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
    report_lines.append("‚Ä¢ –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç
    report_text = "\n".join(report_lines)
    
    report_path = "cifar10_test_report.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report_text)
    
    print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {report_path}")
    
    # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é –≤–µ—Ä—Å–∏—é
    print("\nüìã –ö–†–ê–¢–ö–ò–ô –û–¢–ß–Å–¢:")
    print("-" * 40)
    for line in report_lines[:30]:  # –ü–µ—Ä–≤—ã–µ 30 —Å—Ç—Ä–æ–∫
        print(f"   {line}")
    
    return report_text


# ============================================
# 6. –ì–õ–ê–í–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù
# ============================================

def main():
    """–ì–ª–∞–≤–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ CIFAR-10"""
    print("\n" + "=" * 70)
    print("üöÄ –ó–ê–ü–£–°–ö –ü–ê–ô–ü–õ–ê–ô–ù–ê –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ù–ê CIFAR-10")
    print("=" * 70)
    
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ CIFAR-10 —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º test
        print("\n1Ô∏è‚É£ –≠–¢–ê–ü: –ó–ê–ì–†–£–ó–ö–ê CIFAR-10")
        df, output_dir = load_cifar10_as_dataframe("cifar10_test_dataset", fix_test_size=500)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        original_train_df = df[df['dataset'] == 'train'].copy()
        original_test_df = df[df['dataset'] == 'test'].copy()
        
        # 2. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        print("\n2Ô∏è‚É£ –≠–¢–ê–ü: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–¢–ê–°–ï–¢–ê")
        container = visualize_cifar10_dataset(df, output_dir)
        
        # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å AutoForge
        print("\n3Ô∏è‚É£ –≠–¢–ê–ü: –û–ë–†–ê–ë–û–¢–ö–ê AUTOFORGE")
        print("   ‚Ä¢ Train: —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π (augment_factor=3.0)")
        print("   ‚Ä¢ Test: –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è 500 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
        
        results = test_cifar10_with_autoforge(df, output_dir)
        
        if results is None:
            print("‚ùå AutoForge –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")
            return False
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results['original_train_df'] = original_train_df
        results['original_test_df'] = original_test_df
        results['output_dir'] = output_dir
        
        # 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        print("\n4Ô∏è‚É£ –≠–¢–ê–ü: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        model_results = train_cifar10_models(results)
        
        # 5. –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
        print("\n5Ô∏è‚É£ –≠–¢–ê–ü: –°–û–ó–î–ê–ù–ò–ï –û–¢–ß–Å–¢–ê")
        report = create_cifar10_report(df, results, model_results)
        
        # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML –æ—Ç—á—ë—Ç–∞
        if results and results.get('train_result'):
            print("\n6Ô∏è‚É£ –≠–¢–ê–ü: HTML –û–¢–ß–Å–¢ AUTOFORGE")
            results['train_result'].save_report("cifar10_autoforge_report.html")
            print("‚úÖ HTML –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: cifar10_autoforge_report.html")
        
        # 7. –ò—Ç–æ–≥–∏
        print("\n" + "=" * 70)
        print("üéâ –ü–ê–ô–ü–õ–ê–ô–ù –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ù–ê CIFAR-10 –ó–ê–í–ï–†–®–Å–ù!")
        print("=" * 70)
        
        print("\nüìä –ö–õ–Æ–ß–ï–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   ‚Ä¢ Train –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(original_train_df)} ‚Üí {len(results['train_result'].data)}")
        print(f"   ‚Ä¢ Test –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(original_test_df)} (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)")
        print(f"   ‚Ä¢ –£–≤–µ–ª–∏—á–µ–Ω–∏–µ train: {len(results['train_result'].data)/len(original_train_df):.2f}x")
        print(f"   ‚Ä¢ –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {results['total_time']:.2f} —Å–µ–∫")
        print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {results['train_result'].quality_score:.1%}")
        
        if model_results and model_results.get('proc_results'):
            proc = model_results['proc_results']
            print(f"   ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {proc['final_val_acc']:.2f}%")
            
            if model_results.get('raw_results'):
                raw = model_results['raw_results']
                acc_improvement = proc['final_val_acc'] - raw['final_val_acc']
                print(f"   ‚Ä¢ –£–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏: {acc_improvement:+.2f}%")
        
        print("\nüìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:")
        print("   1. cifar10_test_dataset/ - –î–∞—Ç—ã—Å–µ—Ç CIFAR-10")
        print("   2. cifar10_test_report.txt - –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç")
        print("   3. cifar10_autoforge_report.html - HTML –æ—Ç—á—ë—Ç AutoForge")
        print("   4. cifar10_comparison.png - –ì—Ä–∞—Ñ–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        print("   5. cifar10_samples.png - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print("   –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ automl_data —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================
# –ó–ê–ü–£–°–ö
# ============================================

if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ matplotlib
    plt.rcParams['figure.figsize'] = (12, 8)
    plt.rcParams['font.size'] = 10
    plt.rcParams['axes.grid'] = True
    plt.rcParams['grid.alpha'] = 0.3
    
    # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
    success = main()
    
    if success:
        print("\n‚úÖ –ü–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à—ë–Ω!")
        print("   automl_data –æ—Ç–ª–∏—á–Ω–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å CIFAR-10.")
    else:
        print("\n‚ùå –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–∞–º–∏.")
    
    print("\n" + "=" * 70)